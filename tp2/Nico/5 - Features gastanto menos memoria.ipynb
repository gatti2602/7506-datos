{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Root de archivos de datos\n",
    "root_dir = '../data/'\n",
    "\n",
    "#Lee menos data si es testing\n",
    "is_testing = True\n",
    "testing_frac = .30\n",
    "\n",
    "# Cantidad minima de aparicion de equipos en un dataframe\n",
    "min_devices = 10\n",
    "\n",
    "# Identificador unico de equipos\n",
    "device_uuid = ['ref_hash']\n",
    "\n",
    "# Ventanas de tiempo\n",
    "windows = pd.DataFrame({\n",
    "'window_nr':[1,2,3,4],\n",
    "'n': 1,\n",
    "'min_date': dt.datetime(2019,4,18),\n",
    "'max_date': [dt.datetime(2019,4,21),dt.datetime(2019,4,22),dt.datetime(2019,4,23),dt.datetime(2019,4,24)]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediccion de Tiempos de Arribo\n",
    "===============================\n",
    "\n",
    "Se utilizan los distintos dataframes para armar un set de features que sirvan para predecir el tiempo hasta la aparicion de un dispositivo nuevamente. \n",
    "\n",
    "Este notebook tiene la siguiente estructura: \n",
    "\n",
    "1. Lectura de los Dataframes\n",
    "2. Arreglo de los datos\n",
    "3. Armado de Features\n",
    "4. Armado de labels\n",
    "5. Armado del set de pruebas\n",
    "6. Training del Modelo Predictivo\n",
    "7. Predicciones\n",
    "8. Evaluacion \n",
    "\n",
    "------------------------------\n",
    "## 1. Lectura de los Dataframes\n",
    "\n",
    "Se realiza la carga de los dataframes en memoria para el armado del modelo predictivo. La lectura se hace optimizando los tipos de datos a fin de utilizar la menor cantidad de memoria posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels a submitir con las predicciones\n",
    "\n",
    "to_predict = pd.read_csv(root_dir + 'target_competencia_ids.csv')\n",
    "to_predict = to_predict.apply(lambda x: np.int64(x['ref_hash'][0:x['ref_hash'].find('_')]), axis='columns').drop_duplicates().to_frame()\n",
    "to_predict.columns = ['ref_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_to_drop = pd.read_csv(root_dir+'hash_to_drop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Lectura de Auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Testing, #records: 14222858\n"
     ]
    }
   ],
   "source": [
    "#Optimizado para menos memoria\n",
    "auction_dtypes = {\n",
    "    #'ref_type_id': np.int8,\n",
    "    'device_id': np.int64,\n",
    "    'source_id': np.int8\n",
    "}\n",
    "\n",
    "auctions = pd.read_csv(root_dir + 'auctions.csv.gzip',\n",
    "                       compression = 'gzip',\n",
    "                       dtype = auction_dtypes,\n",
    "                       usecols = list(auction_dtypes.keys()) + ['date'],\n",
    "                       parse_dates = ['date'])\n",
    "auctions.rename({'device_id':'ref_hash'}, inplace=True, axis='columns')\n",
    "auctions['n'] = 1\n",
    "\n",
    "# Para hacer pruebas ocupando menos memoria se hace un sampleo aleatorio de la mitad del dataframe y se elimina el resto\n",
    "if is_testing:\n",
    "    auctions = auctions.sample(frac=testing_frac)\n",
    "    print(\"Is Testing, #records:\", auctions['n'].sum())\n",
    "else:\n",
    "    print(\"Not Testing, #records:\", auctions['n'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Arreglo de datos de Auctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino los ref_hash duplicados en mas de un ref_type\n",
    "auctions = auctions.loc[~auctions['ref_hash'].isin(hash_to_drop['ref_hash'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminados: 790528\n"
     ]
    }
   ],
   "source": [
    "#Elimino los registros con menos de un minimo de entradas, ya que no hay mucho que predecir en estos casos\n",
    "orig_count = auctions['n'].sum()\n",
    "auctions = auctions.groupby(device_uuid, sort = False).filter(lambda x: x['n'].sum() >= min_devices) \n",
    "last_count = auctions['n'].sum()\n",
    "print('Eliminados:', orig_count-last_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions.sort_values(by=(device_uuid+['date']), inplace=True)\n",
    "auctions['next_date'] = auctions.groupby(device_uuid, as_index = False, sort=False)['date'].transform(lambda x: x.shift(-1))\n",
    "auctions = auctions.loc[(~auctions['next_date'].isnull())]\n",
    "auctions['secs_to_next'] = (auctions['next_date'] - auctions['date']).transform(lambda x: round(x.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13017264"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctions['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-04-18 00:00:00.015050')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctions['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-04-26 23:59:57.829179')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctions['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ref_hash</th>\n",
       "      <th>source_id</th>\n",
       "      <th>n</th>\n",
       "      <th>next_date</th>\n",
       "      <th>secs_to_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14875535</th>\n",
       "      <td>2019-04-20 02:52:26.892880</td>\n",
       "      <td>41863526108385</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-20 03:12:26.681062</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42215538</th>\n",
       "      <td>2019-04-20 03:12:26.681062</td>\n",
       "      <td>41863526108385</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-20 03:14:56.144108</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40811049</th>\n",
       "      <td>2019-04-20 03:14:56.144108</td>\n",
       "      <td>41863526108385</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-20 03:15:25.579598</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12654048</th>\n",
       "      <td>2019-04-20 03:15:25.579598</td>\n",
       "      <td>41863526108385</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-20 03:17:35.306737</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21547286</th>\n",
       "      <td>2019-04-20 03:17:35.306737</td>\n",
       "      <td>41863526108385</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-20 03:17:49.278956</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               date        ref_hash  source_id  n  \\\n",
       "14875535 2019-04-20 02:52:26.892880  41863526108385          3  1   \n",
       "42215538 2019-04-20 03:12:26.681062  41863526108385          5  1   \n",
       "40811049 2019-04-20 03:14:56.144108  41863526108385          3  1   \n",
       "12654048 2019-04-20 03:15:25.579598  41863526108385          3  1   \n",
       "21547286 2019-04-20 03:17:35.306737  41863526108385          3  1   \n",
       "\n",
       "                          next_date  secs_to_next  \n",
       "14875535 2019-04-20 03:12:26.681062          1200  \n",
       "42215538 2019-04-20 03:14:56.144108           149  \n",
       "40811049 2019-04-20 03:15:25.579598            29  \n",
       "12654048 2019-04-20 03:17:35.306737           130  \n",
       "21547286 2019-04-20 03:17:49.278956            14  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Read de Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs_dtypes = {\n",
    "    'application_id': np.int32,\n",
    "    #'ref_type': np.int64,\n",
    "    'ref_hash': np.int64, \n",
    "    #'click_hash':'category',\n",
    "    'attributed': 'category',\n",
    "    'implicit': 'category',\n",
    "    #'device_countrycode': 'object', \n",
    "    'device_brand': 'object',\n",
    "    'device_model': 'object', \n",
    "    'session_user_agent': 'object', \n",
    "    'user_agent': 'object', \n",
    "    'event_uuid':'object',\n",
    "    'kind': 'object',\n",
    "    'wifi': 'object', \n",
    "    'trans_id': 'object', \n",
    "    #'ip_address':'object', \n",
    "    'device_language': 'object'\n",
    "}\n",
    "install_cols = list(installs_dtypes.keys()) + ['created']\n",
    "installs = pd.read_csv(root_dir + 'installs.csv.gzip', \n",
    "                       compression='gzip', \n",
    "                       usecols=install_cols,\n",
    "                       dtype= installs_dtypes,\n",
    "                      parse_dates=['created'])\n",
    "\n",
    "installs['n'] = 1\n",
    "print('#records:', installs['n'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Arreglo de datos de Installs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'installs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fcdf504541e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#installs.loc[installs['ref_type'] == 1494519392962156891, 'ref_type'] = 7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'af_app_open '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'af_app_opened'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'af_app_opend'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'af_app_opened'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'installs' is not defined"
     ]
    }
   ],
   "source": [
    "#installs.loc[installs['ref_type'] == 1891515180541284343, 'ref_type'] = 1\n",
    "#installs.loc[installs['ref_type'] == 1494519392962156891, 'ref_type'] = 7\n",
    "\n",
    "installs['kind'] = installs['kind'].str.replace(' ','_')\n",
    "installs['kind'] = installs['kind'].str.replace('af_app_open ','af_app_opened')\n",
    "installs['kind'] = installs['kind'].str.replace('af_app_opend','af_app_opened')\n",
    "installs['kind'] = installs['kind'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['device_brand','device_model','session_user_agent','user_agent','kind','wifi','device_language']:\n",
    "    installs[i] = installs[i].fillna('unknown')\n",
    "installs['device_brand'] = installs['device_brand'].astype('category')\n",
    "installs['device_model'] = installs['device_model'].astype('category')\n",
    "installs['session_user_agent'] = installs['session_user_agent'].astype('category')\n",
    "installs['user_agent'] = installs['user_agent'].astype('category')\n",
    "installs['kind'] = installs['kind'].astype('category')\n",
    "installs['wifi'] = installs['wifi'].astype('category')\n",
    "installs['device_language'] = installs['device_language'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Lectura de Auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_dtypes = {\n",
    "    'advertiser_id': np.int8, \n",
    "    'action_id': np.int32, \n",
    "    'source_id': np.int8, \n",
    "    'latitude' : np.float64, \n",
    "    'longitude': np.float64, \n",
    "    'wifi_connection': np.bool, \n",
    "    'carrier_id': np.int16, \n",
    "    'trans_id': 'object',\n",
    "    'os_minor':'object', \n",
    "    #'agent_device' : 'category', \n",
    "    #'os_major': 'category', \n",
    "    'specs_brand': 'category', \n",
    "    #'brand': np.int8,\n",
    "    'timeToClick': np.float64, \n",
    "    #'touchX': np.float64, \n",
    "    #'touchY': np.float64, \n",
    "    #'ref_type':np.int64, \n",
    "    'ref_hash':np.int64,\n",
    "    'created' : 'object'\n",
    "}\n",
    "clicks = pd.read_csv(root_dir + 'clicks.csv.gzip', \n",
    "                     compression='gzip',\n",
    "                     low_memory=False,\n",
    "                     parse_dates=['created'])\n",
    "clicks['n'] = 1\n",
    "print('#records:', clicks['n'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Testing, #records: 7744581\n"
     ]
    }
   ],
   "source": [
    "events_dtypes = {\n",
    "    'event_id': np.int64,\n",
    "    'ref_type': np.int64,\n",
    "    'ref_hash': np.int64,\n",
    "    'application_id': np.int64,\n",
    "    'attributed': np.bool,\n",
    "    'device_os_version': 'object',\n",
    "    'device_brand': 'object',\n",
    "    'device_model': 'object',\n",
    "    'device_city': 'object',\n",
    "    'session_user_agent': 'object',\n",
    "    'trans_id': 'category',\n",
    "    'user_agent': 'object',\n",
    "    'event_uuid': 'object',\n",
    "    'carrier': 'object',\n",
    "    'kind': 'object',\n",
    "    'device_os': 'object',\n",
    "    'wifi': np.bool,\n",
    "    'connection_type': 'object',\n",
    "    #'ip_address': np.int64,\n",
    "    #'device_language': 'category'\n",
    "}\n",
    "events_cols = list(events_dtypes.keys()) + ['date']\n",
    "events = pd.read_csv(root_dir + 'events.csv.gzip', \n",
    "                     compression='gzip',\n",
    "                     dtype=events_dtypes,\n",
    "                     usecols=events_cols,\n",
    "                     parse_dates=['date'])\n",
    "events['n'] = 1\n",
    "# Para hacer pruebas ocupando menos memoria se hace un sampleo aleatorio de la mitad del dataframe y se elimina el resto\n",
    "if is_testing:\n",
    "    events = events.sample(frac=testing_frac)\n",
    "    print(\"Is Testing, #records:\", events['n'].sum())\n",
    "else:\n",
    "    print(\"Not Testing, #records:\", events['n'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
